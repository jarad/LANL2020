
@article{higdonComputerModelCalibration2008,
	title = {Computer Model Calibration Using High-Dimensional Output},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214507000000888},
	doi = {10.1198/016214507000000888},
	abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimensionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory.},
	pages = {570--583},
	number = {482},
	journaltitle = {Journal of the American Statistical Association},
	author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
	urldate = {2020-09-15},
	date = {2008-06-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214507000000888},
	keywords = {Computer experiments, Functional data analysis, Gaussian process, Prediction, Predictive science, Uncertainty quantification},
	file = {Full Text PDF:/home/enitex/Zotero/storage/DJ3EGIVH/Higdon et al. - 2008 - Computer Model Calibration Using High-Dimensional .pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/HLJDG8L9/016214507000000888.html:text/html}
}

@article{pratolaBayesianAdditiveRegression2016,
	title = {Bayesian Additive Regression Tree Calibration of Complex High-Dimensional Computer Models},
	volume = {58},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2015.1049749},
	doi = {10.1080/00401706.2015.1049749},
	abstract = {Complex natural phenomena are increasingly investigated by the use of a complex computer simulator. To leverage the advantages of simulators, observational data need to be incorporated in a probabilistic framework so that uncertainties can be quantified. A popular framework for such experiments is the statistical computer model calibration experiment. A limitation often encountered in current statistical approaches for such experiments is the difficulty in modeling high-dimensional observational datasets and simulator outputs as well as high-dimensional inputs. As the complexity of simulators seems to only grow, this challenge will continue unabated. In this article, we develop a Bayesian statistical calibration approach that is ideally suited for such challenging calibration problems. Our approach leverages recent ideas from Bayesian additive regression Tree models to construct a random basis representation of the simulator outputs and observational data. The approach can flexibly handle high-dimensional datasets, high-dimensional simulator inputs, and calibration parameters while quantifying important sources of uncertainty in the resulting inference. We demonstrate our methodology on a {CO}2 emissions rate calibration problem, and on a complex simulator of subterranean radionuclide dispersion, which simulates the spatial–temporal diffusion of radionuclides released during nuclear bomb tests at the Nevada Test Site. Supplementary computer code and datasets are available online.},
	pages = {166--179},
	number = {2},
	journaltitle = {Technometrics},
	author = {Pratola, M. T. and Higdon, D. M.},
	urldate = {2020-09-15},
	date = {2016-04-02},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2015.1049749},
	keywords = {Uncertainty quantification, Catastrophe model, Climate change, Markov chain Monte Carlo, Nonparametric, Treaty verification},
	file = {Full Text PDF:/home/enitex/Zotero/storage/MV66QGTP/Pratola and Higdon - 2016 - Bayesian Additive Regression Tree Calibration of C.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/8I2WTDQA/00401706.2015.html:text/html}
}

@article{salterEfficientCalibrationHighdimensional2019,
	title = {Efficient calibration for high-dimensional computer model output using basis methods},
	url = {http://arxiv.org/abs/1906.05758},
	abstract = {Calibration of expensive computer models with high-dimensional output fields can be approached via history matching. If the entire output field is matched, with patterns or correlations between locations or time points represented, calculating the distance metric between observational data and model output for a single input setting requires a time intensive inversion of a high-dimensional matrix. By using a low-dimensional basis representation rather than emulating each output individually, we define a metric in the reduced space that allows the implausibility for the field to be calculated efficiently, with only small matrix inversions required, using projection that is consistent with the variance specifications in the implausibility. We show that projection using the \$L\_2\$ norm can result in different conclusions, with the ordering of points not maintained on the basis, with implications for both history matching and probabilistic methods. We demonstrate the scalability of our method through history matching of the Canadian atmosphere model, {CanAM}4, comparing basis methods to emulation of each output individually, showing that the basis approach can be more accurate, whilst also being more efficient.},
	journaltitle = {{arXiv}:1906.05758 [stat]},
	author = {Salter, James M. and Williamson, Daniel B.},
	urldate = {2020-09-15},
	date = {2019-06-13},
	eprinttype = {arxiv},
	eprint = {1906.05758},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/enitex/Zotero/storage/L5VM55LQ/Salter and Williamson - 2019 - Efficient calibration for high-dimensional compute.pdf:application/pdf;arXiv.org Snapshot:/home/enitex/Zotero/storage/QIND66MU/1906.html:text/html}
}

@article{loeppkyChoosingSampleSize2009,
	title = {Choosing the Sample Size of a Computer Experiment: A Practical Guide},
	volume = {51},
	issn = {0040-1706},
	url = {https://amstat.tandfonline.com/doi/abs/10.1198/TECH.2009.08040},
	doi = {10.1198/TECH.2009.08040},
	shorttitle = {Choosing the Sample Size of a Computer Experiment},
	abstract = {We provide reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process ({GP}). The first characteristic is the total sensitivity of a code output variable to all input variables; the second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (i.e., effect sparsity). Both measures relate directly to the correlation structure in the {GP} approximation of the code. In this way, the article moves toward a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein.},
	pages = {366--376},
	number = {4},
	journaltitle = {Technometrics},
	shortjournal = {null},
	author = {Loeppky, Jason L. and Sacks, Jerome and Welch, William J.},
	urldate = {2020-09-15},
	date = {2009-11-01},
	note = {Publisher: Taylor \& Francis},
	file = {Snapshot:/home/enitex/Zotero/storage/3J2YH4Z3/TECH.2009.html:text/html;Submitted Version:/home/enitex/Zotero/storage/3WBX86G8/Loeppky et al. - 2009 - Choosing the Sample Size of a Computer Experiment.pdf:application/pdf}
}

@article{bayarriFrameworkValidationComputer2007,
	title = {A Framework for Validation of Computer Models},
	volume = {49},
	issn = {0040-1706},
	url = {https://doi.org/10.1198/004017007000000092},
	doi = {10.1198/004017007000000092},
	abstract = {We present a framework that enables computer model evaluation oriented toward answering the question: Does the computer model adequately represent reality? The proposed validation framework is a six-step procedure based on Bayesian and likelihood methodology. The Bayesian methodology is particularly well suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models, combining multiple sources of information, and updating validation assessments as new information is acquired. Moreover, it allows inferential statements to be made about predictive error associated with model predictions in untested situations. The framework is implemented in a test bed example of resistance spot welding, to provide context for each of the six steps in the proposed validation process.},
	pages = {138--154},
	number = {2},
	journaltitle = {Technometrics},
	author = {Bayarri, Maria J. and Berger, James O. and Paulo, Rui and Sacks, Jerry and Cafeo, John A. and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
	urldate = {2020-09-15},
	date = {2007-05-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/004017007000000092},
	keywords = {Prediction, Bayesian analysis, Identifiability, Model discrepancy},
	file = {Full Text PDF:/home/enitex/Zotero/storage/FI8AKUGH/Bayarri et al. - 2007 - A Framework for Validation of Computer Models.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/KSUVIKBF/004017007000000092.html:text/html}
}

@article{loeppkyChoosingSampleSize2009a,
	title = {Choosing the Sample Size of a Computer Experiment: A Practical Guide},
	volume = {51},
	issn = {0040-1706},
	url = {https://amstat.tandfonline.com/doi/abs/10.1198/TECH.2009.08040},
	doi = {10.1198/TECH.2009.08040},
	shorttitle = {Choosing the Sample Size of a Computer Experiment},
	abstract = {We provide reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process ({GP}). The first characteristic is the total sensitivity of a code output variable to all input variables; the second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (i.e., effect sparsity). Both measures relate directly to the correlation structure in the {GP} approximation of the code. In this way, the article moves toward a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein.},
	pages = {366--376},
	number = {4},
	journaltitle = {Technometrics},
	shortjournal = {null},
	author = {Loeppky, Jason L. and Sacks, Jerome and Welch, William J.},
	urldate = {2020-09-15},
	date = {2009-11-01},
	note = {Publisher: Taylor \& Francis},
	file = {Snapshot:/home/enitex/Zotero/storage/X5AJ6E4T/TECH.2009.html:text/html;Submitted Version:/home/enitex/Zotero/storage/NQ6TJ56G/Loeppky et al. - 2009 - Choosing the Sample Size of a Computer Experiment.pdf:application/pdf}
}

@article{contiBayesianEmulationComplex2010,
	title = {Bayesian emulation of complex multi-output and dynamic computer models},
	volume = {140},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375809002559},
	doi = {10.1016/j.jspi.2009.08.006},
	abstract = {Computer models are widely used in scientific research to study and predict the behaviour of complex systems. The run times of computer-intensive simulators are often such that it is impractical to make the thousands of model runs that are conventionally required for sensitivity analysis, uncertainty analysis or calibration. In response to this problem, highly efficient techniques have recently been developed based on a statistical meta-model (the emulator) that is built to approximate the computer model. The approach, however, is less straightforward for dynamic simulators, designed to represent time-evolving systems. Generalisations of the established methodology to allow for dynamic emulation are here proposed and contrasted. Advantages and difficulties are discussed and illustrated with an application to the Sheffield Dynamic Global Vegetation Model, developed within the {UK} Centre for Terrestrial Carbon Dynamics.},
	pages = {640--651},
	number = {3},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Conti, Stefano and O’Hagan, Anthony},
	urldate = {2020-09-15},
	date = {2010-03-01},
	langid = {english},
	keywords = {Computer experiments, Bayesian inference, Dynamic models, Hierarchical models},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/KPU8XNDS/S0378375809002559.html:text/html}
}

@article{rougierEfficientEmulatorsMultivariate2008,
	title = {Efficient Emulators for Multivariate Deterministic Functions},
	volume = {17},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/106186008X384032},
	doi = {10.1198/106186008X384032},
	abstract = {One of the challenges with emulating the response of a multivariate function to its inputs is the quantity of data that must be assimilated, which is the product of the number of model evaluations and the number of outputs. This article shows how even large calculations can be made tractable. It is already appreciated that gains can be made when the emulator residual covariance function is treated as separable in the model-inputs and model-outputs. Here, an additional simplification on the structure of the regressors in the emulator mean function allows very substantial further gains. The result is that it is now possible to emulate rapidly—on a desktop computer—models with hundreds of evaluations and hundreds of outputs. This is demonstrated through calculating costs in floating-point operations, and in an illustration. Even larger sets of outputs are possible if they have additional structure, for example, spatial-temporal.},
	pages = {827--843},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Rougier, Jonathan},
	urldate = {2020-09-15},
	date = {2008-12-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/106186008X384032},
	keywords = {Gaussian process, Kronecker product, Outer-product emulator, Separable variance},
	file = {Full Text PDF:/home/enitex/Zotero/storage/VBVEWDHF/Rougier - 2008 - Efficient Emulators for Multivariate Deterministic.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/IXIXFGYH/106186008X384032.html:text/html}
}

@article{higdonBayesianCalibrationApproach2008,
	title = {A Bayesian calibration approach to the thermal problem},
	volume = {197},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782507005087},
	doi = {10.1016/j.cma.2007.05.031},
	series = {Validation Challenge Workshop},
	abstract = {Many of the problems we work with at Los Alamos National Laboratory are similar to the thermal problem described in the tasking document. In this paper, we describe the tools and methods we have developed that utilize experimental data and detailed physics simulations for uncertainty quantification, and apply them to the thermal challenge problem. We then go on to address the regulatory question posed in the problem description. This statistical framework used here is largely based on the approach of Kennedy and O’Hagan [Kennedy, M., O’Hagan, A., Bayesian calibration of computer models (with discussion), J. Royal Stat. Soc. B 68 (2001) 425–464], but has been extended to deal with functional output of the simulation model.},
	pages = {2431--2441},
	number = {29},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Higdon, Dave and Nakhleh, Charles and Gattiker, James and Williams, Brian},
	urldate = {2020-09-15},
	date = {2008-05-01},
	langid = {english},
	keywords = {Computer experiments, Functional data analysis, Gaussian process, Predictive science, Uncertainty quantification, Certification, Predictability, Verification and validation},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/VZ625RHR/S0045782507005087.html:text/html;Submitted Version:/home/enitex/Zotero/storage/SMZI339E/Higdon et al. - 2008 - A Bayesian calibration approach to the thermal pro.pdf:application/pdf}
}

@article{frickerMultivariateGaussianProcess2013,
	title = {Multivariate Gaussian Process Emulators With Nonseparable Covariance Structures},
	volume = {55},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2012.715835},
	doi = {10.1080/00401706.2012.715835},
	abstract = {The Gaussian process regression model is a popular type of “emulator” used as a fast surrogate for computationally expensive simulators (deterministic computer models). For simulators with multivariate output, common practice is to specify a separable covariance structure for the Gaussian process. Though computationally convenient, this can be too restrictive, leading to poor performance of the emulator, particularly when the different simulator outputs represent different physical quantities. Also, treating the simulator outputs as independent can lead to inappropriate representations of joint uncertainty. We develop nonseparable covariance structures for Gaussian process emulators, based on the linear model of coregionalization and convolution methods. Using two case studies, we compare the performance of these covariance structures both with standard separable covariance structures and with emulators that assume independence between the outputs. In each case study, we find that only emulators with nonseparable covariances structures have sufficient flexibility both to give good predictions and to represent joint uncertainty about the simulator outputs appropriately. This article has supplementary material online.},
	pages = {47--56},
	number = {1},
	journaltitle = {Technometrics},
	author = {Fricker, Thomas E. and Oakley, Jeremy E. and Urban, Nathan M.},
	urldate = {2020-09-15},
	date = {2013-02-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2012.715835},
	keywords = {Computer experiment, Convolved process, Coregionalization, Metamodel},
	file = {Full Text PDF:/home/enitex/Zotero/storage/2UPSW5BW/Fricker et al. - 2013 - Multivariate Gaussian Process Emulators With Nonse.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/8TSQ6MV7/00401706.2012.html:text/html}
}

@article{bilionisMultioutputSeparableGaussian2013,
	title = {Multi-output separable Gaussian process: Towards an efficient, fully Bayesian paradigm for uncertainty quantification},
	volume = {241},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999113000417},
	doi = {10.1016/j.jcp.2013.01.011},
	shorttitle = {Multi-output separable Gaussian process},
	abstract = {Computer codes simulating physical systems usually have responses that consist of a set of distinct outputs (e.g., velocity and pressure) that evolve also in space and time and depend on many unknown input parameters (e.g., physical constants, initial/boundary conditions, etc.). Furthermore, essential engineering procedures such as uncertainty quantification, inverse problems or design are notoriously difficult to carry out mostly due to the limited simulations available. The aim of this work is to introduce a fully Bayesian approach for treating these problems which accounts for the uncertainty induced by the finite number of observations. Our model is built on a multi-dimensional Gaussian process that explicitly treats correlations between distinct output variables as well as space and/or time. The proper use of a separable covariance function enables us to describe the huge covariance matrix as a Kronecker product of smaller matrices leading to efficient algorithms for carrying out inference and predictions. The novelty of this work, is the recognition that the Gaussian process model defines a posterior probability measure on the function space of possible surrogates for the computer code and the derivation of an algorithmic procedure that allows us to sample it efficiently. We demonstrate how the scheme can be used in uncertainty quantification tasks in order to obtain error bars for the statistics of interest that account for the finite number of observations.},
	pages = {212--239},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Bilionis, Ilias and Zabaras, Nicholas and Konomi, Bledar A. and Lin, Guang},
	urldate = {2020-09-15},
	date = {2013-05-15},
	langid = {english},
	keywords = {Gaussian process, Uncertainty quantification, Kronecker product, Bayesian, Separable covariance function, Stochastic partial differential equations, Surrogate models},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/W256XLGY/S0021999113000417.html:text/html}
}

@article{bilionisMultioutputLocalGaussian2012,
	title = {Multi-output local Gaussian process regression: Applications to uncertainty quantification},
	volume = {231},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999112002513},
	doi = {10.1016/j.jcp.2012.04.047},
	shorttitle = {Multi-output local Gaussian process regression},
	abstract = {We develop an efficient, Bayesian Uncertainty Quantification framework using a novel treed Gaussian process model. The tree is adaptively constructed using information conveyed by the observed data about the length scales of the underlying process. On each leaf of the tree, we utilize Bayesian Experimental Design techniques in order to learn a multi-output Gaussian process. The constructed surrogate can provide analytical point estimates, as well as error bars, for the statistics of interest. We numerically demonstrate the effectiveness of the suggested framework in identifying discontinuities, local features and unimportant dimensions in the solution of stochastic differential equations.},
	pages = {5718--5746},
	number = {17},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Bilionis, Ilias and Zabaras, Nicholas},
	urldate = {2020-09-15},
	date = {2012-07-01},
	langid = {english},
	keywords = {Gaussian process, Uncertainty quantification, Bayesian, Stochastic partial differential equations, Adaptivity, Multi-element, Multi-output},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/87NNVV5N/S0021999112002513.html:text/html;Bilionis and Zabaras - 2012 - Multi-output local Gaussian process regression Ap.pdf:/home/enitex/Zotero/storage/YX4W3XW5/Bilionis and Zabaras - 2012 - Multi-output local Gaussian process regression Ap.pdf:application/pdf}
}

@article{arendtImprovingIdentifiabilityModel2012,
	title = {Improving Identifiability in Model Calibration Using Multiple Responses},
	volume = {134},
	issn = {1050-0472},
	url = {https://asmedigitalcollection.asme.org/mechanicaldesign/article/134/10/100909/467354/Improving-Identifiability-in-Model-Calibration},
	doi = {10.1115/1.4007573},
	number = {10},
	journaltitle = {Journal of Mechanical Design},
	shortjournal = {J. Mech. Des},
	author = {Arendt, Paul D. and Apley, Daniel W. and Chen, Wei and Lamb, David and Gorsich, David},
	urldate = {2020-09-15},
	date = {2012-10-01},
	langid = {english},
	note = {Publisher: American Society of Mechanical Engineers Digital Collection},
	file = {Snapshot:/home/enitex/Zotero/storage/QABWYJHB/467354.html:text/html;Arendt et al. - 2012 - Improving Identifiability in Model Calibration Usi.pdf:/home/enitex/Zotero/storage/B33YW9T6/Arendt et al. - 2012 - Improving Identifiability in Model Calibration Usi.pdf:application/pdf}
}

@incollection{santnerPhysicalExperimentsComputer2018,
	location = {New York, {NY}},
	title = {Physical Experiments and Computer Experiments},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_1},
	series = {Springer Series in Statistics},
	abstract = {Experiments have long been used to study the relationship between a set of inputs to a physical system and the resulting output. Termed physical experiments in this text, there is a growing trend to replace or supplement the physical system used in such an experiment with a deterministic simulator.},
	pages = {1--26},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_1},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/GK73D3XE/Santner et al. - 2018 - Physical Experiments and Computer Experiments.pdf:application/pdf}
}

@incollection{santnerStochasticProcessModels2018,
	location = {New York, {NY}},
	title = {Stochastic Process Models for Describing Computer Simulator Output},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_2},
	series = {Springer Series in Statistics},
	abstract = {Recall from Chap. 1 that {\textbackslash}({\textbackslash}boldsymbol\{x\}{\textbackslash}) denotes a generic input to our computer simulator and {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) denotes the associated output. This chapter will introduce several classes of random function models for {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) that will serve as the core building blocks for the interpolators, experimental designs, calibration, and tuning methodologies that will be introduced in later chapters. The reason that the random function approach is so useful is that accurate prediction based on black box computer simulator output requires a rich class of {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) options when only a minimal amount might be known about the output function. Indeed, regression mean modeling of simulator output is usually based on a rather arbitrarily selected parametric form.},
	pages = {27--66},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_2},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/MLBZSPKS/Santner et al. - 2018 - Stochastic Process Models for Describing Computer .pdf:application/pdf}
}

@incollection{santnerBayesianInferenceSimulator2018,
	location = {New York, {NY}},
	title = {Bayesian Inference for Simulator Output},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_4},
	series = {Springer Series in Statistics},
	abstract = {In Chap. 3 the correlation and precision parameters are completely unknown for the process model assumed to generate simulator output. In contrast this chapter assumes that the researcher has prior knowledge about the unknown parameters that is quantifiable in the form of a prior distribution.},
	pages = {115--143},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_4},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/8LY4F3FS/Santner et al. - 2018 - Bayesian Inference for Simulator Output.pdf:application/pdf}
}

@incollection{santnerEmpiricalBestLinear2018,
	location = {New York, {NY}},
	title = {Empirical Best Linear Unbiased Prediction of Computer Simulator Output},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_3},
	series = {Springer Series in Statistics},
	abstract = {This chapter and Chap. 4 discuss techniques for predicting output for a computer simulator based on “training” runs from the model. Knowing how to predict computer output is a prerequisite for answering most practical research questions that involve computer simulators including those listed in Sect. 1.3. As an example where the prediction methods described below will be central, Chap. 6 will present a sequential design for a computer experiment to find input conditions {\textbackslash}({\textbackslash}boldsymbol\{x\}{\textbackslash}) that maximize a computer output which requires prediction of {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) at all untried sites.},
	pages = {67--114},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_3},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/LU5YNSAP/Santner et al. - 2018 - Empirical Best Linear Unbiased Prediction of Compu.pdf:application/pdf}
}

@incollection{santnerSpaceFillingDesignsComputer2018,
	location = {New York, {NY}},
	title = {Space-Filling Designs for Computer Experiments},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_5},
	series = {Springer Series in Statistics},
	abstract = {This chapter and the next discuss how to select inputs at which to compute the output of a computer experiment to achieve specific goals. The inputs one selects constitute the “experimental design.” As in previous chapters, the inputs are referred to as “runs.” The region corresponding to the values of the inputs that is to be studied is called the experimental region. A point in this region corresponds to a specific set of values of the inputs. Thus, an experimental design is a specification of points (runs) in the experimental region at which the response is to be computed.},
	pages = {145--200},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_5},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/37IWBFKN/Santner et al. - 2018 - Space-Filling Designs for Computer Experiments.pdf:application/pdf}
}

@incollection{santnerCriterionBasedExperimentalDesigns2018,
	location = {New York, {NY}},
	title = {Some Criterion-Based Experimental Designs},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_6},
	series = {Springer Series in Statistics},
	abstract = {Chapter 5 considered designs that attempt to spread observations “evenly” throughout the experimental region. Such designs were called space-filling designs. Recall that one rationale for using a space-filling design is the following. If it is believed that interesting features of the true model are just as likely to be in one part of the input region as another, observations should be taken in all portions of the input region. There are many heuristic criteria for producing designs that might be considered space-filling; several of these were discussed in Chap. 5. However none of the methods was tied to a statistical justification, and no single criterion was singled out as best.},
	pages = {201--246},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_6},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/SDE4DFN7/Santner et al. - 2018 - Some Criterion-Based Experimental Designs.pdf:application/pdf}
}

@incollection{santnerSensitivityAnalysisVariable2018,
	location = {New York, {NY}},
	title = {Sensitivity Analysis and Variable Screening},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_7},
	series = {Springer Series in Statistics},
	abstract = {This chapter discusses sensitivity analysis and the related topic of variable screening. The setup is as follows. A vector of inputs {\textbackslash}({\textbackslash}boldsymbol\{x\} = (x\_\{1\},{\textbackslash}ldots,x\_\{d\}){\textbackslash}) is given which potentially affects a “response” function {\textbackslash}(y({\textbackslash}boldsymbol\{x\}) = y(x\_\{1\},{\textbackslash}ldots,x\_\{d\}){\textbackslash}). Sensitivity analysis seeks to quantify how variation in {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) can be apportioned to the inputs x1, …, xd and to the interactions among these inputs.},
	pages = {247--297},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_7},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/9X8TVERR/Santner et al. - 2018 - Sensitivity Analysis and Variable Screening.pdf:application/pdf}
}

@incollection{santnerCalibration2018,
	location = {New York, {NY}},
	title = {Calibration},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_8},
	series = {Springer Series in Statistics},
	abstract = {Ideally, every computer simulator should be calibrated using observations from the physical system that is modeled by the simulator. Roughly, calibration uses data from dual simulator and physical system platforms to estimate, with uncertainty, the unknown values of the calibration inputs that govern the physical system (and which can be set in the simulator).},
	pages = {299--379},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_8},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/FZ6G59M3/Santner et al. - 2018 - Calibration.pdf:application/pdf}
}

@online{DesignAnalysisComputer,
	title = {Design and Analysis of Computer Experiments on {JSTOR}},
	url = {https://www.jstor.org/stable/2245858?seq=1#metadata_info_tab_contents},
	urldate = {2020-09-15},
	file = {Design and Analysis of Computer Experiments on JSTOR:/home/enitex/Zotero/storage/FLLXH48F/2245858.html:text/html}
}

@article{muehlenstaedtComputerExperimentsFunctional2017,
	title = {Computer experiments with functional inputs and scalar outputs by a norm-based approach},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-016-9672-z},
	doi = {10/ghbndr},
	abstract = {A framework for designing and analyzing computer experiments is presented, which is constructed for dealing with functional and scalar inputs and scalar outputs. For designing experiments with both functional and scalar inputs, a two-stage approach is suggested. The ﬁrst stage consists of constructing a candidate set for each functional input. During the second stage, an optimal combination of the found candidate sets and a Latin hypercube for the scalar inputs is sought. The resulting designs can be considered to be generalizations of Latin hypercubes. Gaussian process models are explored as metamodel. The functional inputs are incorporated into the Kriging model by applying norms in order to deﬁne distances between two functional inputs. We propose the use of B-splines to make the calculation of these norms computationally feasible.},
	pages = {1083--1097},
	number = {4},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
	urldate = {2020-09-15},
	date = {2017-07},
	langid = {english},
	file = {Muehlenstaedt et al. - 2017 - Computer experiments with functional inputs and sc.pdf:/home/enitex/Zotero/storage/LMBP2YJB/Muehlenstaedt et al. - 2017 - Computer experiments with functional inputs and sc.pdf:application/pdf}
}

@article{muehlenstaedtComputerExperimentsFunctional2017a,
	title = {Computer experiments with functional inputs and scalar outputs by a norm-based approach},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9672-z},
	doi = {10/ghbndr},
	abstract = {A framework for designing and analyzing computer experiments is presented, which is constructed for dealing with functional and scalar inputs and scalar outputs. For designing experiments with both functional and scalar inputs, a two-stage approach is suggested. The first stage consists of constructing a candidate set for each functional input. During the second stage, an optimal combination of the found candidate sets and a Latin hypercube for the scalar inputs is sought. The resulting designs can be considered to be generalizations of Latin hypercubes. Gaussian process models are explored as metamodel. The functional inputs are incorporated into the Kriging model by applying norms in order to define distances between two functional inputs. We propose the use of B-splines to make the calculation of these norms computationally feasible.},
	pages = {1083--1097},
	number = {4},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/CK9PISPW/Muehlenstaedt et al. - 2017 - Computer experiments with functional inputs and sc.pdf:application/pdf}
}

@article{morrisGaussianSurrogatesComputer2012,
	title = {Gaussian Surrogates for Computer Models With Time-Varying Inputs and Outputs},
	volume = {54},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2012.648870},
	doi = {10/ghbnds},
	abstract = {Computer models of dynamic systems produce outputs that are functions of time; models that solve systems of differential equations often have this character. In many cases, time series output can be usefully reduced via principal components to simplify analysis. Time-indexed inputs, such as the functions that describe time-varying boundary conditions, are also common with such models. However, inputs that are functions of time often do not have one or a few “characteristic shapes” that are more common with output functions, and so, principal component representation has less potential for reducing the dimension of input functions. In this article, Gaussian process surrogates are described for models with inputs and outputs that are both functions of time. The focus is on construction of an appropriate covariance structure for such surrogates, some experimental design issues, and an application to a model of marrow cell dynamics.},
	pages = {42--50},
	number = {1},
	journaltitle = {Technometrics},
	author = {Morris, Max D.},
	urldate = {2020-09-15},
	date = {2012-02-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2012.648870},
	keywords = {Computer experiment, Dynamic model, Gaussian stochastic process, Maximin distance design, Meta-model.},
	file = {Full Text PDF:/home/enitex/Zotero/storage/M2C8IYD7/Morris - 2012 - Gaussian Surrogates for Computer Models With Time-.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/QYM53SYP/00401706.2012.html:text/html}
}

@article{fruthSequentialDesignsSensitivity2015,
	title = {Sequential designs for sensitivity analysis of functional inputs in computer experiments},
	volume = {134},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832014001732},
	doi = {10/f6w37d},
	abstract = {Computer experiments are nowadays commonly used to analyze industrial processes aiming at achieving a wanted outcome. Sensitivity analysis plays an important role in exploring the actual impact of adjustable parameters on the response variable. In this work we focus on sensitivity analysis of a scalar-valued output of a time-consuming computer code depending on scalar and functional input parameters. We investigate a sequential methodology, based on piecewise constant functions and sequential bifurcation, which is both economical and fully interpretable. The new approach is applied to a sheet metal forming problem in three sequential steps, resulting in new insights into the behavior of the forming process over time.},
	pages = {260--267},
	journaltitle = {Reliability Engineering \& System Safety},
	shortjournal = {Reliability Engineering \& System Safety},
	author = {Fruth, J. and Roustant, O. and Kuhnt, S.},
	urldate = {2020-09-15},
	date = {2015-02-01},
	langid = {english},
	keywords = {Functional input, Sensitivity analysis, Sequential approach, Sheet metal forming, Springback},
	file = {Fruth et al. - 2015 - Sequential designs for sensitivity analysis of fun.pdf:/home/enitex/Zotero/storage/JNFPJ48W/Fruth et al. - 2015 - Sequential designs for sensitivity analysis of fun.pdf:application/pdf}
}

@article{morrisDecomposingFunctionalModel2018,
	title = {Decomposing Functional Model Inputs for Variance-Based Sensitivity Analysis},
	volume = {6},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/abs/10.1137/18M1173058},
	doi = {10/ghbnd7},
	abstract = {Variance-based sensitivity analysis is a popular technique for assessing the importance of model inputs when there are natural or meaningful probability distributions associated with each input. This approach can be used when some of the model inputs are functions rather than scalar valued, but may be somewhat less useful in this case because it does not address the nature of the relationships between functional inputs and model outputs. We consider the option of separating a random function-valued input, represented by a vector of relatively high dimension, into one or a few scalar-valued summaries that are suggested by the context of the modeling exercise, and an independent, high-dimensional “residual.” The first case we discuss is for inputs that are realizations of Gaussian processes, where the summary statistics are linear functionals of the input, and the residual can always be defined to be statistically independent of these. The second case is for input functions that might be described as “pulses” occurring in simulated time as a Poisson process, where the summary statistic is the number of such pulses and all other details form the residual. The third case involves periodic input functions for which the overall scale of the Fourier coefficients is controlled by the scalar-valued summary. We conclude by describing a graphical technique that may help to identify useful low-dimensional function summaries. When the model output is more sensitive to the low-dimensional summaries than to the residuals, this is useful information concerning the nature of model sensitivity, and also provides a route to constructing model surrogates with scalar-valued indices that accurately represent most of the variation in the output.},
	pages = {1584--1599},
	number = {4},
	journaltitle = {{SIAM}/{ASA} Journal on Uncertainty Quantification},
	shortjournal = {{SIAM}/{ASA} J. Uncertainty Quantification},
	author = {Morris, Max D.},
	urldate = {2020-09-15},
	date = {2018-01-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Full Text PDF:/home/enitex/Zotero/storage/WCMTC9UA/Morris - 2018 - Decomposing Functional Model Inputs for Variance-B.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/W7PLWQTU/18M1173058.html:text/html}
}

@article{morrisMaximinDistanceOptimal2014,
	title = {Maximin distance optimal designs for computer experiments with time-varying inputs and outputs},
	volume = {144},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375812002996},
	doi = {10/ghbnd8},
	series = {International Conference on Design of Experiments},
	abstract = {Computer models of dynamic systems produce outputs that are functions of time; models that solve systems of differential equations often have this character. Time-indexed inputs, such as the functions that describe time-varying boundary conditions, are also common with such models. Morris (2012) described a generalization of the Gaussian process often used to produce “meta-models” when inputs are finite-dimensional vectors, that can be used in the functional input setting, and showed how the maximin distance design optimality criterion (Johnson et al., 1990) can also be extended to this case. This paper describes an upper bound on the maximin distance criterion for functional inputs. A class of designs that are optimal under certain conditions is also presented; while these designs are of limited practical value, they show that the derived bound cannot be improved in the general case.},
	pages = {63--68},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Morris, Max D.},
	urldate = {2020-09-15},
	date = {2014-01-01},
	langid = {english},
	keywords = {Computer experiment, Dynamic model, Gaussian stochastic process, Maximin distance design, Meta-model, Surrogate},
	file = {Morris - 2014 - Maximin distance optimal designs for computer expe.pdf:/home/enitex/Morris - 2014 - Maximin distance optimal designs for computer expe.pdf:application/pdf}
}

@article{drigneiEmpiricalBayesianAnalysis2006,
	title = {Empirical Bayesian Analysis for High-Dimensional Computer Output},
	volume = {48},
	issn = {0040-1706},
	url = {https://doi.org/10.1198/004017005000000472},
	doi = {10/cvfj99},
	abstract = {This article proposes a two-stage statistical method for the analysis of multivariate computer experiments when at least one of the output dimensions is large. The stage-one data are modeled by a multivariate extension of a widely used scalar statistical model for computer output. Conditioned on stage-one data, a simple statistical model is then proposed for the stage-two data. The method is demonstrated in a geophysical application involving an ocean model.},
	pages = {230--240},
	number = {2},
	journaltitle = {Technometrics},
	author = {Drignei, Dorin},
	urldate = {2020-09-15},
	date = {2006-05-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/004017005000000472},
	keywords = {Computer experiment, Discrete-time Brownian bridge, Gaussian correlation, Geophysical model},
	file = {Full Text PDF:/home/enitex/Zotero/storage/9F4IXCT5/Drignei - 2006 - Empirical Bayesian Analysis for High-Dimensional C.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/MY4T98RC/004017005000000472.html:text/html}
}

@article{contiGaussianProcessEmulation2009,
	title = {Gaussian process emulation of dynamic computer codes},
	volume = {96},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/27798855},
	doi = {10/fsh9h2},
	abstract = {Computer codes are used in scientific research to study and predict the behaviour of complex systems. Their run times often make uncertainty and sensitivity analyses impractical because of the thousands of runs that are conventionally required, so efficient techniques have been developed based on a statistical representation of the code. The approach is less straightforward for dynamic codes, which represent time-evolving systems. We develop a novel iterative system to build a statistical model of dynamic computer codes, which is demonstrated on a rainfall-runoff simulator.},
	pages = {663--676},
	number = {3},
	journaltitle = {Biometrika},
	author = {{CONTI}, S. and {GOSLING}, J. P. and {OAKLEY}, J. E. and O'{HAGAN}, A.},
	urldate = {2020-09-15},
	date = {2009},
	note = {Publisher: [Oxford University Press, Biometrika Trust]}
}

@book{gramacySurrogatesGaussianProcess2020,
	location = {Boca Raton, Florida},
	title = {Surrogates: Gaussian process modeling, design and optimization for the applied sciences},
	publisher = {Chapman Hall/{CRC}},
	author = {Gramacy, Robert B.},
	date = {2020},
	file = {Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:/home/enitex/Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:application/pdf;Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:/home/enitex/Zotero/storage/QY4N4DRK/Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:application/pdf}
}

@article{janssenNewGenerationAgricultural2017,
	title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
	volume = {155},
	issn = {0308-521X},
	url = {http://www.sciencedirect.com/science/article/pii/S0308521X16305637},
	doi = {10/gbnmtk},
	shorttitle = {Towards a new generation of agricultural system data, models and knowledge products},
	abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology ({ICT}). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and {ICT} technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.},
	pages = {200--212},
	journaltitle = {Agricultural Systems},
	shortjournal = {Agricultural Systems},
	author = {Janssen, Sander J. C. and Porter, Cheryl H. and Moore, Andrew D. and Athanasiadis, Ioannis N. and Foster, Ian and Jones, James W. and Antle, John M.},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	keywords = {Agricultural models, Big data, {ICT}, Linked data, Open science, Sensing, Visualization},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/3PS6WJ53/S0308521X16305637.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/YRR9AGDX/Janssen et al. - 2017 - Towards a new generation of agricultural system da.pdf:application/pdf}
}

@article{holzworthAPSIMEvolutionNew2014,
	title = {{APSIM} – Evolution towards a new generation of agricultural systems simulation},
	volume = {62},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815214002102},
	doi = {10/gddc8w},
	abstract = {Agricultural systems models worldwide are increasingly being used to explore options and solutions for the food security, climate change adaptation and mitigation and carbon trading problem domains. {APSIM} (Agricultural Production Systems {sIMulator}) is one such model that continues to be applied and adapted to this challenging research agenda. From its inception twenty years ago, {APSIM} has evolved into a framework containing many of the key models required to explore changes in agricultural landscapes with capability ranging from simulation of gene expression through to multi-field farms and beyond. Keating et al. (2003) described many of the fundamental attributes of {APSIM} in detail. Much has changed in the last decade, and the {APSIM} community has been exploring novel scientific domains and utilising software developments in social media, web and mobile applications to provide simulation tools adapted to new demands. This paper updates the earlier work by Keating et al. (2003) and chronicles the changing external challenges and opportunities being placed on {APSIM} during the last decade. It also explores and discusses how {APSIM} has been evolving to a “next generation” framework with improved features and capabilities that allow its use in many diverse topics.},
	pages = {327--350},
	journaltitle = {Environmental Modelling \& Software},
	shortjournal = {Environmental Modelling \& Software},
	author = {Holzworth, Dean P. and Huth, Neil I. and {deVoil}, Peter G. and Zurcher, Eric J. and Herrmann, Neville I. and {McLean}, Greg and Chenu, Karine and van Oosterom, Erik J. and Snow, Val and Murphy, Chris and Moore, Andrew D. and Brown, Hamish and Whish, Jeremy P. M. and Verrall, Shaun and Fainges, Justin and Bell, Lindsay W. and Peake, Allan S. and Poulton, Perry L. and Hochman, Zvi and Thorburn, Peter J. and Gaydon, Donald S. and Dalgliesh, Neal P. and Rodriguez, Daniel and Cox, Howard and Chapman, Scott and Doherty, Alastair and Teixeira, Edmar and Sharp, Joanna and Cichota, Rogerio and Vogeler, Iris and Li, Frank Y. and Wang, Enli and Hammer, Graeme L. and Robertson, Michael J. and Dimes, John P. and Whitbread, Anthony M. and Hunt, James and van Rees, Harm and {McClelland}, Tim and Carberry, Peter S. and Hargreaves, John N. G. and {MacLeod}, Neil and {McDonald}, Cam and Harsdorf, Justin and Wedgwood, Sara and Keating, Brian A.},
	urldate = {2020-09-15},
	date = {2014-12-01},
	langid = {english},
	keywords = {Agricultural systems, {APSIM}, Crop, Farming system, Gene-to-phenotype model, Model, Simulation},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/MZ4Q48MZ/S1364815214002102.html:text/html}
}

@article{janssenNewGenerationAgricultural2017a,
	title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
	volume = {155},
	issn = {0308-521X},
	url = {http://www.sciencedirect.com/science/article/pii/S0308521X16305637},
	doi = {10/gbnmtk},
	shorttitle = {Towards a new generation of agricultural system data, models and knowledge products},
	abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology ({ICT}). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and {ICT} technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.},
	pages = {200--212},
	journaltitle = {Agricultural Systems},
	shortjournal = {Agricultural Systems},
	author = {Janssen, Sander J. C. and Porter, Cheryl H. and Moore, Andrew D. and Athanasiadis, Ioannis N. and Foster, Ian and Jones, James W. and Antle, John M.},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	keywords = {Agricultural models, Big data, {ICT}, Linked data, Open science, Sensing, Visualization},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/9X926UTT/S0308521X16305637.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/Y7Z2BUBZ/Janssen et al. - 2017 - Towards a new generation of agricultural system da.pdf:application/pdf}
}

@article{janssenNewGenerationAgricultural2017b,
	title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
	volume = {155},
	issn = {0308-521X},
	url = {http://www.sciencedirect.com/science/article/pii/S0308521X16305637},
	doi = {10/gbnmtk},
	shorttitle = {Towards a new generation of agricultural system data, models and knowledge products},
	abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology ({ICT}). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and {ICT} technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.},
	pages = {200--212},
	journaltitle = {Agricultural Systems},
	shortjournal = {Agricultural Systems},
	author = {Janssen, Sander J. C. and Porter, Cheryl H. and Moore, Andrew D. and Athanasiadis, Ioannis N. and Foster, Ian and Jones, James W. and Antle, John M.},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	keywords = {Agricultural models, Big data, {ICT}, Linked data, Open science, Sensing, Visualization},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/RMBHJYET/S0308521X16305637.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/RERAEPU9/Janssen et al. - 2017 - Towards a new generation of agricultural system da.pdf:application/pdf}
}

@article{currinBayesianPredictionDeterministic1991,
	title = {Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experiments},
	volume = {86},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475138},
	doi = {10/ggktqx},
	abstract = {This article is concerned with prediction of a function y(t) over a (multidimensional) domain T, given the function values at a set of “sites” t (1), t (2), …, t (n) in T, and with the design, that is, with the selection of those sites. The motivating application is the design and analysis of computer experiments, where t determines the input to a computer model of a physical or behavioral system, and y(t) is a response that is part of the output or is calculated from it. Following a Bayesian formulation, prior uncertainty about the function y is expressed by means of a random function Y, which is taken here to be a Gaussian stochastic process. The mean of the posterior process can be used as the prediction function ŷ(t), and the variance can be used as a measure of uncertainty. This kind of approach has been used previously in Bayesian interpolation and is strongly related to the kriging methods used in geostatistics. Here emphasis is placed on product linear and product cubic correlation functions, which yield prediction functions that are, respectively, linear or cubic splines in every dimension. A posterior entropy criterion is adopted for design; this minimizes the expected uncertainty about the posterior process, as measured by the entropy. A computational algorithm for finding entropy-optimal designs on multidimensional grids is described. Several examples are presented, including a two-dimensional experiment on a computer model of a thermal energy storage device and a six-dimensional experiment on an integrated circuit simulator. Predictions are made using several different families of correlation functions, with parameters chosen to maximize the likelihood. For comparison, predictions are also made via least squares fitting of various polynomial and spline models. The Bayesian design/prediction methods, which do not require any modeling of y, produce comparatively good predictions. For some correlation functions, however, the 95\% posterior probability intervals do not give adequate coverage of the true values of y at selected test sites. These methods are fairly simple and offer considerable potential for virtually automatic implementation, although further development is needed before they can be applied routinely in practice.},
	pages = {953--963},
	number = {416},
	journaltitle = {Journal of the American Statistical Association},
	author = {Currin, Carla and Mitchell, Toby and Morris, Max and Ylvisaker, Don},
	urldate = {2020-09-15},
	date = {1991-12-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1991.10475138},
	keywords = {Computer models, Correlation function, Cross-validation, Entropy, Experimental design, Interpolation, Kriging, Optimal design, Spline fitting, Stochastic processes},
	file = {Full Text PDF:/home/enitex/Zotero/storage/U4WYQUHY/Currin et al. - 1991 - Bayesian Prediction of Deterministic Functions, wi.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/CBIWADT6/01621459.1991.html:text/html}
}

@article{sacksDesignAnalysisComputer1989,
	title = {Design and Analysis of Computer Experiments},
	volume = {4},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1177012413},
	doi = {10/fn276g},
	abstract = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic--rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient prediction. With this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
	pages = {409--423},
	number = {4},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Sacks, Jerome and Welch, William J. and Mitchell, Toby J. and Wynn, Henry P.},
	urldate = {2020-09-15},
	date = {1989-11},
	mrnumber = {MR1041765},
	zmnumber = {0955.62619},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Experimental design, computer-aided design, kriging, response surface, spatial statistics},
	file = {Full Text PDF:/home/enitex/Zotero/storage/3T963M3I/Sacks et al. - 1989 - Design and Analysis of Computer Experiments.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/YGHKPND3/1177012413.html:text/html}
}

@book{santnerDesignAnalysisComputer2018,
	location = {New York, {NY}},
	edition = {2nd ed. 2018},
	title = {The Design and Analysis of Computer Experiments},
	isbn = {978-1-4939-8847-1},
	series = {Springer Series in Statistics},
	abstract = {This book describes methods for designing and analyzing experiments that are conducted using a computer code, a computer experiment, and, when possible, a physical experiment. Computer experiments continue to increase in popularity as surrogates for and adjuncts to physical experiments. Since the publication of the first edition, there have been many methodological advances and software developments to implement these new methodologies. The computer experiments literature has emphasized the construction of algorithms for various data analysis tasks (design construction, prediction, sensitivity analysis, calibration among others), and the development of web-based repositories of designs for immediate application. While it is written at a level that is accessible to readers with Masters-level training in Statistics, the book is written in sufficient detail to be useful for practitioners and researchers. New to this revised and expanded edition: - An expanded presentation of basic material on computer experiments and Gaussian processes with additional simulations and examples - A new comparison of plug-in prediction methodologies for real-valued simulator output - An enlarged discussion of space-filling designs including Latin Hypercube designs ({LHDs}), near-orthogonal designs, and nonrectangular regions - A chapter length description of process-based designs for optimization, to improve good overall fit, quantile estimation, and Pareto optimization - A new chapter describing graphical and numerical sensitivity analysis tools - Substantial new material on calibration-based prediction and inference for calibration parameters - Lists of software that can be used to fit models discussed in the book to aid practitioners},
	pagetotal = {1},
	publisher = {Springer New York : Imprint: Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	date = {2018},
	doi = {10.1007/978-1-4939-8847-1},
	keywords = {Applied mathematics, Engineering mathematics, Mathematical and Computational Engineering, Statistical Theory and Methods, Statistics, Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences},
	file = {Santner et al. - 2018 - The Design and Analysis of Computer Experiments.pdf:/home/enitex/Zotero/storage/XDI63R68/Santner et al. - 2018 - The Design and Analysis of Computer Experiments.pdf:application/pdf}
}

@article{gartonScorebasedLikelihoodRatios2020,
	title = {Score-based likelihood ratios and sparse Gaussian processes},
	url = {https://lib.dr.iastate.edu/etd/17938},
	doi = {10/ghbt2c},
	journaltitle = {Graduate Theses and Dissertations},
	author = {Garton, Nathaniel},
	date = {2020-01-01},
	file = {"Score-based likelihood ratios and sparse Gaussian processes" by Nathaniel Morrissey Garton:/home/enitex/Zotero/storage/J4TPWJ8V/17938.html:text/html;Full Text:/home/enitex/Zotero/storage/Y9WURCN3/Garton - 2020 - Score-based likelihood ratios and sparse Gaussian .pdf:application/pdf}
}

@article{gartonKnotSelectionSparse2020,
	title = {Knot selection in sparse Gaussian processes with a variational objective function},
	volume = {13},
	rights = {© 2020 The Authors. Statistical Analysis and Data Mining published by Wiley Periodicals {LLC}},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11459},
	doi = {10/ghbt2d},
	abstract = {Sparse, knot-based Gaussian processes have enjoyed considerable success as scalable approximations of full Gaussian processes. Certain sparse models can be derived through specific variational approximations to the true posterior, and knots can be selected to minimize the Kullback-Leibler divergence between the approximate and true posterior. While this has been a successful approach, simultaneous optimization of knots can be slow due to the number of parameters being optimized. Furthermore, there have been few proposed methods for selecting the number of knots, and no experimental results exist in the literature. We propose using a one-at-a-time knot selection algorithm based on Bayesian optimization to select the number and locations of knots. We showcase the competitive performance of this method relative to optimization of knots simultaneously on three benchmark datasets, but at a fraction of the computational cost.},
	pages = {324--336},
	number = {4},
	journaltitle = {Statistical Analysis and Data Mining: The {ASA} Data Science Journal},
	author = {Garton, Nathaniel and Niemi, Jarad and Carriquiry, Alicia},
	urldate = {2020-09-16},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11459},
	keywords = {knot selection, machine learning, nonparametric regression, sparse Gaussian processes, variational inference},
	file = {Snapshot:/home/enitex/Zotero/storage/KQFRWLVM/sam.html:text/html;Full Text PDF:/home/enitex/Zotero/storage/HH35NCDK/Garton et al. - 2020 - Knot selection in sparse Gaussian processes with a.pdf:application/pdf}
}